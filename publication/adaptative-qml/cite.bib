@article{Moussa_2023,
doi = {10.1088/2058-9565/acef55},
url = {https://dx.doi.org/10.1088/2058-9565/acef55},
year = {2023},
month = {aug},
publisher = {IOP Publishing},
volume = {8},
number = {4},
pages = {045019},
author = {Charles Moussa and Max Hunter Gordon and Michal Baczyk and M Cerezo and Lukasz Cincio and Patrick J Coles},
title = {Resource frugal optimizer for quantum machine learning},
journal = {Quantum Science and Technology},
abstract = {Quantum-enhanced data science, also known as quantum machine learning (QML), is of growing interest as an application of near-term quantum computers. Variational QML algorithms have the potential to solve practical problems on real hardware, particularly when involving quantum data. However, training these algorithms can be challenging and calls for tailored optimization procedures. Specifically, QML applications can require a large shot-count overhead due to the large datasets involved. In this work, we advocate for simultaneous random sampling over both the dataset as well as the measurement operators that define the loss function. We consider a highly general loss function that encompasses many QML applications, and we show how to construct an unbiased estimator of its gradient. This allows us to propose a shot-frugal gradient descent optimizer called Refoqus (REsource Frugal Optimizer for QUantum Stochastic gradient descent). Our numerics indicate that Refoqus can save several orders of magnitude in shot cost, even relative to optimizers that sample over measurement operators alone.}
}